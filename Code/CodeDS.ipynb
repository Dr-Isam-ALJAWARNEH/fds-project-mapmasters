{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the pygeohash library, which allows encoding and decoding of geohashes (used for spatial indexing and grouping)\n",
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5ahOQs7Q-KC"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "# os is used for handling file paths and directory operations\n",
    "import os\n",
    "# pandas is used for data manipulation and analysis using DataFrames\n",
    "import pandas as pd\n",
    "# numpy provides support for numerical operations and array handling\n",
    "import numpy as np\n",
    "# geopandas extends pandas to handle geospatial data using shapefiles, geometry columns, etc.\n",
    "import geopandas as gpd\n",
    "# DBSCAN is a clustering algorithm from scikit-learn used for density-based clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "# MinMaxScaler is used to scale features to a specified range, typically [0, 1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# mean_absolute_error is used to evaluate prediction performance by calculating average absolute errors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# great_circle is used to calculate the shortest distance between two points on the Earth's surface\n",
    "from geopy.distance import great_circle\n",
    "# pygeohash is used for encoding latitude and longitude into geohash strings for spatial grouping\n",
    "import pygeohash as pgh\n",
    "# folium is used for interactive map visualization in Python\n",
    "import folium\n",
    "# MarkerCluster allows grouping markers in folium maps for better readability\n",
    "from folium.plugins import MarkerCluster\n",
    "# matplotlib is a standard plotting library for creating static, animated, and interactive visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# seaborn is a statistical data visualization library based on matplotlib, providing enhanced plots\n",
    "import seaborn as sns\n",
    "# shapely.geometry.Point is used to create point objects for spatial operations\n",
    "from shapely.geometry import Point\n",
    "# nearest_points is used to find the nearest point between two geometries\n",
    "from shapely.ops import nearest_points\n",
    "# google.colab.drive is used to mount Google Drive in Colab to access files stored there\n",
    "from google.colab import drive\n",
    "# Calculates the Silhouette Score, which is used to evaluate the quality of clustering results. A higher score indicates better-defined clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Splits the dataset into training and testing subsets. This is typically used in machine learning to evaluate model performance by training on one set and testing on another.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import folium for interactive map visualization of spatial data\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Drive & Load AQ and Taxi Data\n",
    "# Mount Google Drive to access files stored in the Drive\n",
    "drive.mount('/content/drive')\n",
    "# Set the path to the directory where the datasets are stored\n",
    "base_path = '/content/drive/MyDrive/Datasets/'\n",
    "\n",
    "# Initialize an empty list to collect data from each AQ CSV file\n",
    "aq_dfs = []\n",
    "# Loop through AQ_data_1.csv to AQ_data_19.csv to read each file\n",
    "for i in range(1, 20):\n",
    "    # Construct the file path for each AQ dataset\n",
    "    file_path = os.path.join(base_path, f\"AQ_data_{i}.csv\")\n",
    "\n",
    "    # Check if the file exists at the specified path\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename the necessary columns for standardization\n",
    "        df = df.rename(columns={\n",
    "            'Latitude': 'latitude',\n",
    "            'Longitude': 'longitude',\n",
    "            'ReadingDateTimeUTC': 'timestamp',\n",
    "            'PM25': 'pm25'\n",
    "        })\n",
    "\n",
    "        # Keep only the relevant columns needed for analysis\n",
    "        df = df[['latitude', 'longitude', 'timestamp', 'pm25']]\n",
    "\n",
    "        # Print how many records were loaded from the current file\n",
    "        print(f\" Loaded AQ_data_{i}.csv with {len(df)} records\")\n",
    "\n",
    "        # Append the DataFrame to the AQ list\n",
    "        aq_dfs.append(df)\n",
    "    else:\n",
    "        # If the file doesn't exist, print a message\n",
    "        print(f\" File not found: {file_path}\")\n",
    "\n",
    "# Combine all individual AQ DataFrames into a single DataFrame\n",
    "aq_df = pd.concat(aq_dfs, ignore_index=True)\n",
    "\n",
    "# Display a preview of the first few records of the combined AQ data\n",
    "print(\" Preview of Combined AQ Data:\")\n",
    "print(aq_df.head())\n",
    "\n",
    "# Print the total number of AQ records loaded\n",
    "print(f\" Total AQ Records Loaded: {len(aq_df)}\")\n",
    "\n",
    "# Load All Taxi Trips Data\n",
    "# Construct the full file path to the Taxi_Trips dataset\n",
    "taxi_file_path = os.path.join(base_path, 'Taxi_Trips.csv')\n",
    "\n",
    "# Load selected columns from the taxi dataset for memory efficiency\n",
    "taxi_df = pd.read_csv(taxi_file_path, usecols=[\n",
    "    'Trip Start Timestamp',\n",
    "    'Trip Seconds',\n",
    "    'Trip Miles',\n",
    "    'Pickup Centroid Latitude',\n",
    "    'Pickup Centroid Longitude'\n",
    "])\n",
    "\n",
    "# Rename the columns for consistency with other datasets\n",
    "taxi_df = taxi_df.rename(columns={\n",
    "    'Trip Start Timestamp': 'trip_start_timestamp',\n",
    "    'Pickup Centroid Latitude': 'pickup_latitude',\n",
    "    'Pickup Centroid Longitude': 'pickup_longitude',\n",
    "    'Trip Seconds': 'trip_seconds',\n",
    "    'Trip Miles': 'trip_miles'\n",
    "})\n",
    "\n",
    "# Display a preview of the taxi data\n",
    "print(\"\\n Preview of Taxi Trips Data:\")\n",
    "print(taxi_df.head())\n",
    "\n",
    "# Print the total number of taxi trip records loaded\n",
    "print(f\" Total Taxi Records Loaded: {len(taxi_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert Timestamps & Resample by 5-Hour Windows\n",
    "# Convert AQ 'timestamp' column from string format to pandas datetime format\n",
    "aq_df['timestamp'] = pd.to_datetime(aq_df['timestamp'])\n",
    "\n",
    "# Print a few converted AQ timestamps to verify the format\n",
    "print(\"AQ timestamps converted:\")\n",
    "print(aq_df[['timestamp']].head())\n",
    "\n",
    "# Convert Taxi 'trip_start_timestamp' column from string to datetime format\n",
    "taxi_df['trip_start_timestamp'] = pd.to_datetime(taxi_df['trip_start_timestamp'])\n",
    "\n",
    "# Print a few converted Taxi timestamps to verify the format\n",
    "print(\"\\nTaxi trip timestamps converted:\")\n",
    "print(taxi_df[['trip_start_timestamp']].head())\n",
    "\n",
    "# Round each AQ timestamp down to the nearest 5-hour interval (e.g., 00:00, 05:00, 10:00...)\n",
    "aq_df['time_block'] = aq_df['timestamp'].dt.floor('5H')\n",
    "\n",
    "# Show the original and new 5-hour rounded timestamps for AQ data\n",
    "print(\"\\nAQ data after assigning to 5-hour time blocks:\")\n",
    "print(aq_df[['timestamp', 'time_block']].head())\n",
    "\n",
    "# Round each Taxi trip timestamp to the nearest 5-hour block\n",
    "taxi_df['time_block'] = taxi_df['trip_start_timestamp'].dt.floor('5H')\n",
    "\n",
    "# Show original and new 5-hour rounded timestamps for Taxi data\n",
    "print(\"\\nTaxi data after assigning to 5-hour time blocks:\")\n",
    "print(taxi_df[['trip_start_timestamp', 'time_block']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Geohash Grouping and Feature Aggregation\n",
    "# Define a helper function to compute geohashes based on latitude and longitude\n",
    "def geohash_group(df, lat_col, lon_col, precision=5):\n",
    "    \"\"\"\n",
    "    Adds a 'geohash' column to the DataFrame based on latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        lat_col (str): Name of the latitude column\n",
    "        lon_col (str): Name of the longitude column\n",
    "        precision (int): Geohash precision level (higher = smaller area)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'geohash' column added\n",
    "    \"\"\"\n",
    "    # Apply geohash encoding to each row using specified lat/lon columns\n",
    "    df['geohash'] = df.apply(lambda row: pgh.encode(row[lat_col], row[lon_col], precision=precision), axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply geohash encoding to AQ data\n",
    "aq_df = geohash_group(aq_df, 'latitude', 'longitude')\n",
    "\n",
    "# Apply geohash encoding to Taxi data\n",
    "taxi_df = geohash_group(taxi_df, 'pickup_latitude', 'pickup_longitude')\n",
    "\n",
    "# Print a sample of AQ geohash assignments\n",
    "print(\" Sample AQ geohashes:\")\n",
    "print(aq_df[['latitude', 'longitude', 'geohash']].head())\n",
    "\n",
    "# Print a sample of Taxi geohash assignments\n",
    "print(\"\\n Sample Taxi geohashes:\")\n",
    "print(taxi_df[['pickup_latitude', 'pickup_longitude', 'geohash']].head())\n",
    "\n",
    "# Aggregate taxi data: count number of trips per geohash and time block (trip density)\n",
    "taxi_density = taxi_df.groupby(['geohash', 'time_block']).size().reset_index(name='taxi_density')\n",
    "print(\"\\n Taxi density (trips per geohash and time block):\")\n",
    "print(taxi_density.head())\n",
    "\n",
    "# Aggregate AQ data: calculate average PM2.5 per geohash and time block\n",
    "aq_avg = aq_df.groupby(['geohash', 'time_block'])['pm25'].mean().reset_index(name='pm25')\n",
    "print(\"\\n Average PM2.5 per geohash and time block:\")\n",
    "print(aq_avg.head())\n",
    "\n",
    "# Merge the two datasets on common geohash and time block to align spatial-temporal features\n",
    "features_df = pd.merge(taxi_density, aq_avg, on=['geohash', 'time_block'])\n",
    "\n",
    "# Print the merged features combining taxi activity and air quality\n",
    "print(\"\\n Merged Features DataFrame (Taxi + AQ):\")\n",
    "print(features_df.head())\n",
    "\n",
    "# Show total number of records in the final dataset\n",
    "print(f\" Total Records in Feature Set: {len(features_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute Weighted Feature for Clustering\n",
    "\n",
    "# Define the weight (lambda) that determines the importance of air quality in the combined feature\n",
    "lambda_ = 0.6  # Weight for air quality influence (60% AQ, 40% taxi density)\n",
    "\n",
    "# Create a new weighted feature that blends PM2.5 air quality and taxi density\n",
    "# This will be used as a combined metric for clustering\n",
    "features_df['weighted_feature'] = lambda_ * features_df['pm25'] + (1 - lambda_) * features_df['taxi_density']\n",
    "\n",
    "# Decode the geohash back into latitude and longitude coordinates\n",
    "# This step is useful for visualizing or mapping the clusters later\n",
    "features_df[['latitude', 'longitude']] = features_df['geohash'].apply(lambda g: pd.Series(pgh.decode(g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Normalize and Cluster Using DBSCAN and KMeans + Evaluate\n",
    "# Step 5.1: Select features for clustering\n",
    "# We're clustering based on longitude, latitude, and a weighted feature (e.g., combining taxi density + PM2.5)\n",
    "X = features_df[['longitude', 'latitude', 'weighted_feature']].values\n",
    "\n",
    "# Step 5.2: Normalize the features to a [0,1] range using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5.3: Apply DBSCAN clustering\n",
    "# DBSCAN is good for finding arbitrarily shaped clusters and dealing with noise\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "features_df['dbscan_cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Step 5.4: Apply KMeans clustering for comparison\n",
    "# KMeans assumes spherical clusters and needs a fixed number of clusters (n_clusters)\n",
    "kmeans = KMeans(n_clusters=8, random_state=42, n_init='auto')  # You can tune n_clusters\n",
    "features_df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 5.5: Evaluate clustering using Silhouette Score\n",
    "# Silhouette Score ranges from -1 to 1; higher is better\n",
    "# DBSCAN: Ignore noise points labeled as -1\n",
    "dbscan_mask = features_df['dbscan_cluster'] != -1\n",
    "if dbscan_mask.sum() > 1:\n",
    "    dbscan_score = silhouette_score(X_scaled[dbscan_mask], features_df['dbscan_cluster'][dbscan_mask])\n",
    "else:\n",
    "    dbscan_score = -1  # Not enough clusters for scoring\n",
    "\n",
    "# KMeans: Score is calculated on all points\n",
    "kmeans_score = silhouette_score(X_scaled, features_df['kmeans_cluster'])\n",
    "\n",
    "print(f\" Silhouette Score - DBSCAN: {dbscan_score:.4f}\")\n",
    "print(f\" Silhouette Score - KMeans: {kmeans_score:.4f}\")\n",
    "\n",
    "# Step 5.6: Stratified Sampling Based on Geohash\n",
    "# This helps ensure train/test split maintains similar spatial distribution\n",
    "train_df, test_df = train_test_split(\n",
    "    features_df, test_size=0.3, stratify=features_df['geohash'], random_state=42\n",
    ")\n",
    "print(f\" Train size: {len(train_df)}, Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Determine Sensor Candidate Locations (Centroids)\n",
    "# Group the data by DBSCAN cluster labels and compute the mean longitude, latitude, and PM2.5 for each cluster.\n",
    "# These means represent the centroids—ideal candidate locations for placing sensors.\n",
    "cluster_centroids = features_df.groupby('dbscan_cluster')[['longitude', 'latitude', 'pm25']].mean().reset_index()\n",
    "\n",
    "# Exclude noise points labeled as -1 by DBSCAN, as they don't belong to any meaningful cluster.\n",
    "cluster_centroids = cluster_centroids[cluster_centroids['dbscan_cluster'] != -1]\n",
    "\n",
    "# Save the resulting centroids to a CSV file for further analysis or deployment.\n",
    "cluster_centroids.to_csv('sensor_candidates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Accuracy Evaluation \n",
    "# This function evaluates the accuracy of the sensor candidate locations by comparing their PM2.5 values\n",
    "# to the nearest actual ground truth air quality readings (from aq_df).\n",
    "def evaluate_sensor_accuracy(cluster_centers, ground_truth_df):\n",
    "    errors = []\n",
    "    for _, center in cluster_centers.iterrows():\n",
    "        center_point = (center['latitude'], center['longitude'])\n",
    "\n",
    "        # For each cluster center, compute the distance to all ground truth points\n",
    "        ground_truth_df['distance'] = ground_truth_df.apply(\n",
    "            lambda row: great_circle(center_point, (row['latitude'], row['longitude'])).meters,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Find the nearest ground truth point to the cluster center\n",
    "        nearest = ground_truth_df.loc[ground_truth_df['distance'].idxmin()]\n",
    "\n",
    "        # Compare PM2.5 at the center with the nearest ground truth PM2.5 value\n",
    "        predicted_pm25 = center['pm25']\n",
    "        true_pm25 = nearest['pm25']\n",
    "        errors.append(abs(predicted_pm25 - true_pm25))  # Store the absolute error\n",
    "\n",
    "    return np.mean(errors)  # Return the Mean Absolute Error (MAE)\n",
    "\n",
    "# Evaluate the sensor candidate locations and print the MAE\n",
    "mae = evaluate_sensor_accuracy(cluster_centroids, aq_df)\n",
    "print(f\"Mean Absolute Error of sensor placement: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Enrich with External Datasets\n",
    "# Convert cluster_centroids to GeoDataFrame\n",
    "gdf_clusters = gpd.GeoDataFrame(\n",
    "    cluster_centroids,\n",
    "    geometry=[Point(xy) for xy in zip(cluster_centroids.longitude, cluster_centroids.latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Load external geospatial layers \n",
    "base_path = '/content/drive/MyDrive/Datasets/' \n",
    "\n",
    "gdf_bus_stops = gpd.read_file(os.path.join(base_path, \"CTA_BusStops.shp\"))\n",
    "# Urban_Farms data is in a CSV and needs to be converted to a GeoDataFrame:\n",
    "urban_farms_df = pd.read_csv(os.path.join(base_path, \"Urban_Farms.csv\")) # Read as DataFrame using pandas\n",
    "# If 'Urban_Farms.csv' has geometry info, create a GeoDataFrame:\n",
    "gdf_urban_farms = gpd.GeoDataFrame(urban_farms_df, geometry=gpd.points_from_xy(urban_farms_df.LONGITUDE, urban_farms_df.LATITUDE), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "# Load Rail Lines data\n",
    "rail_lines_df = pd.read_csv(os.path.join(base_path, \"Rail_Lines.csv\"))\n",
    "# Convert 'the_geom' column to Shapely geometry objects\n",
    "rail_lines_df['geometry'] = gpd.GeoSeries.from_wkt(rail_lines_df['the_geom'])\n",
    "# Create GeoDataFrame using the converted geometry column\n",
    "gdf_rail_lines = gpd.GeoDataFrame(rail_lines_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "#  Alt_Fuel_Stations.csv has latitude and longitude columns\n",
    "alt_fuel_df = pd.read_csv(os.path.join(base_path, \"Alt_Fuel_Stations.csv\")) \n",
    "# Create a GeoDataFrame from the DataFrame\n",
    "gdf_alt_fuel = gpd.GeoDataFrame(alt_fuel_df, geometry=gpd.points_from_xy(alt_fuel_df.Longitude, alt_fuel_df.Latitude), crs=\"EPSG:4326\") # Assuming 'Longitude' and 'Latitude' are column names\n",
    "\n",
    "\n",
    "# 'Census_Tracts.csv' has geometry information in 'the_geom' column\n",
    "# If it has latitude and longitude, replace 'the_geom' with gpd.points_from_xy(census_df.longitude, census_df.latitude)\n",
    "# Load the census data first as a regular DataFrame\n",
    "census_df = pd.read_csv(os.path.join(base_path, \"Census_Tracts.csv\"))\n",
    "gdf_census = gpd.GeoDataFrame(census_df, geometry=gpd.GeoSeries.from_wkt(census_df['the_geom']), crs=\"EPSG:4326\") # Use census_df here\n",
    "\n",
    "\n",
    "df_wind = pd.read_csv(os.path.join(base_path, \"Beach_Weather_Stations.csv\"))\n",
    "# Add coordinates by station name\n",
    "station_coords = {\n",
    "    \"63rd Street Beach\": (41.780992, -87.572619),\n",
    "    \"Foster Beach\": (41.976816, -87.651546),\n",
    "    \"Montrose Beach\": (41.961708, -87.638719),\n",
    "    \"Oak Street Beach\": (41.910337, -87.626123),\n",
    "    \"Ohio Street Beach\": (41.894000, -87.613000),\n",
    "    \"Rainbow Beach\": (41.761000, -87.565000),\n",
    "    \"North Avenue Beach\": (41.911000, -87.628000)\n",
    "}\n",
    "\n",
    "def get_coords(station_name):\n",
    "    return pd.Series(station_coords.get(station_name, (None, None)), index=['Latitude', 'Longitude'])\n",
    "\n",
    "df_wind[['Latitude', 'Longitude']] = df_wind['Station Name'].apply(get_coords)\n",
    "\n",
    "\n",
    "# Reproject everything to UTM for accurate spatial operations\n",
    "to_utm = lambda gdf: gdf.to_crs(epsg=32616)\n",
    "gdf_clusters_utm = to_utm(gdf_clusters)\n",
    "gdf_bus_stops = to_utm(gdf_bus_stops)\n",
    "gdf_urban_farms = to_utm(gdf_urban_farms)\n",
    "gdf_rail_lines = to_utm(gdf_rail_lines)\n",
    "gdf_alt_fuel = to_utm(gdf_alt_fuel)\n",
    "gdf_census = gdf_census.to_crs(\"EPSG:4326\")  # keep in WGS84 for join\n",
    "\n",
    "# 1. Count of bus stops within 250m \n",
    "buffer = gdf_clusters_utm.copy()\n",
    "buffer['geometry'] = buffer.geometry.buffer(250)\n",
    "joined = gpd.sjoin(gdf_bus_stops, buffer, how=\"inner\", predicate='within')\n",
    "counts = joined.groupby('index_right').size()\n",
    "gdf_clusters_utm['bus_stop_count'] = gdf_clusters_utm.index.map(counts).fillna(0).astype(int)\n",
    "\n",
    "# 2. Distance to nearest urban farm \n",
    "gdf_clusters_utm['dist_urban_farm'] = gdf_clusters_utm.geometry.apply(\n",
    "    lambda pt: gdf_urban_farms.distance(pt).min()\n",
    ")\n",
    "\n",
    "# 3. Distance to nearest rail line \n",
    "gdf_clusters_utm['dist_rail_line'] = gdf_clusters_utm.geometry.apply(\n",
    "    lambda pt: gdf_rail_lines.distance(pt).min()\n",
    ")\n",
    "\n",
    "# 4. Distance to nearest alternative fuel station \n",
    "gdf_clusters_utm['dist_alt_fuel'] = gdf_clusters_utm.geometry.apply(\n",
    "    lambda pt: gdf_alt_fuel.distance(pt).min()\n",
    ")\n",
    "\n",
    "# 5. Add Census vulnerability index (spatial join) \n",
    "# Convert cluster_centroids to GeoDataFrame\n",
    "gdf_clusters = gpd.GeoDataFrame(\n",
    "    cluster_centroids,\n",
    "    geometry=[Point(xy) for xy in zip(cluster_centroids.longitude, cluster_centroids.latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Bring 'geohash' from features_df before reprojection\n",
    "# This assumes features_df has a 'geohash' column corresponding to each cluster centroid\n",
    "# You might need to adjust the merge logic if there's no direct correspondence\n",
    "gdf_clusters = gdf_clusters.merge(features_df[['geohash', 'longitude', 'latitude']], on=['longitude', 'latitude'], how='left')\n",
    "\n",
    "\n",
    "# Merge with features_df to get 'time_block' \n",
    "gdf_clusters = gdf_clusters.merge(\n",
    "    features_df[['geohash', 'time_block']],\n",
    "    left_on='geohash',\n",
    "    right_on='geohash',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# the actual column name is 'Vulnerability_Score'\n",
    "vulnerability_col_name = 'COMMAREA_N'\n",
    "gdf_enriched = gpd.sjoin(\n",
    "    gdf_clusters,\n",
    "    gdf_census[['geometry', vulnerability_col_name]],\n",
    "    how='left',\n",
    "    predicate='within'\n",
    ")\n",
    "print(\" External layers integrated with cluster data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Visualize Sensor Locations with Folium\n",
    "# Create map centered on Chicago\n",
    "m = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "#  cluster_centroids is a DataFrame with your cluster data\n",
    "# Convert cluster_centroids to a GeoDataFrame if it's not already\n",
    "gdf_clusters = gpd.GeoDataFrame(\n",
    "    cluster_centroids,\n",
    "    geometry=[Point(xy) for xy in zip(cluster_centroids.longitude, cluster_centroids.latitude)],\n",
    "    crs=\"EPSG:4326\"  # Assuming your data is in WGS84\n",
    ")\n",
    "\n",
    "# Add sensor locations from your cluster GeoDataFrame\n",
    "for _, row in gdf_clusters.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Time Block: {row.get('time_block', 'N/A')}\" #add a time block info\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save and display\n",
    "m.save(\"sensor_locations_map.html\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"sensor_locations_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Visualization (DBSCAN & KMeans)\n",
    "\n",
    "# Set seaborn style for better aesthetics in plots\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "# Plot DBSCAN clustering results using longitude and latitude\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='longitude', y='latitude',\n",
    "    hue='dbscan_cluster', data=features_df,\n",
    "    palette='Set1', legend='full'\n",
    ")\n",
    "plt.title('DBSCAN Clusters')  # Title for the DBSCAN plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Position the legend outside the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot KMeans clustering results using longitude and latitude\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x='longitude', y='latitude',\n",
    "    hue='kmeans_cluster', data=features_df,\n",
    "    palette='Set2', legend='full'\n",
    ")\n",
    "plt.title('KMeans Clusters')  # Title for the KMeans plot\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Position the legend outside the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of eps values to test for DBSCAN clustering\n",
    "eps_values = [0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "\n",
    "# Initialize a list to store silhouette scores for each eps\n",
    "scores = []\n",
    "\n",
    "# Iterate through each eps value\n",
    "for eps in eps_values:\n",
    "    # Create a DBSCAN model with current eps and fixed min_samples=5\n",
    "    db = DBSCAN(eps=eps, min_samples=5)\n",
    "\n",
    "    # Fit DBSCAN and get cluster labels\n",
    "    labels = db.fit_predict(X_scaled)\n",
    "\n",
    "    # Create a mask to exclude noise points (label = -1)\n",
    "    mask = labels != -1\n",
    "\n",
    "    # If there are enough non-noise points, calculate silhouette score\n",
    "    if mask.sum() > 1:\n",
    "        # Compute silhouette score for current clustering (excluding noise)\n",
    "        score = silhouette_score(X_scaled[mask], labels[mask])\n",
    "    else:\n",
    "        # Assign a default low score if too few points to compute a valid score\n",
    "        score = -1\n",
    "\n",
    "    # Store the score in the list\n",
    "    scores.append(score)\n",
    "\n",
    "    # Print the current eps value and its corresponding silhouette score\n",
    "    print(f\"eps={eps} => Silhouette Score: {score:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning n_clusters for KMeans:\n",
    "range_n = range(2, 11)  # Set the range of cluster numbers (from 2 to 10) to evaluate KMeans with different numbers of clusters\n",
    "kmeans_scores = []  # Initialize an empty list to store silhouette scores for each k\n",
    "\n",
    "for k in range_n:  # Iterate over the range of possible cluster numbers (2 to 10)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')  # Initialize KMeans with k clusters and fixed random state for reproducibility\n",
    "    labels = kmeans.fit_predict(X_scaled)  # Fit the KMeans model to the scaled data and predict cluster labels\n",
    "    score = silhouette_score(X_scaled, labels)  # Calculate the silhouette score to measure clustering quality\n",
    "    kmeans_scores.append(score)  # Append the silhouette score to the list\n",
    "    print(f\"k={k} => Silhouette Score: {score:.4f}\")  # Print the silhouette score for the current value of k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Scores to Choose Best Parameters\n",
    "# Create a new figure for plotting the DBSCAN Silhouette Scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the Silhouette Scores for different 'eps' values (DBSCAN) with markers at each point\n",
    "plt.plot(eps_values, scores, marker='o', label='DBSCAN')\n",
    "\n",
    "# Set the title of the plot to indicate it's showing Silhouette Score vs eps for DBSCAN\n",
    "plt.title('Silhouette Score vs eps (DBSCAN)')\n",
    "\n",
    "# Label the x-axis as 'eps' (epsilon value for DBSCAN)\n",
    "plt.xlabel('eps')\n",
    "\n",
    "# Label the y-axis as 'Silhouette Score'\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "# Enable grid lines for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the legend to indicate the line represents DBSCAN\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Create another figure for plotting the KMeans Silhouette Scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the Silhouette Scores for different 'n_clusters' values (KMeans) with markers at each point\n",
    "plt.plot(list(range_n), kmeans_scores, marker='o', color='orange', label='KMeans')\n",
    "\n",
    "# Set the title of the plot to indicate it's showing Silhouette Score vs n_clusters for KMeans\n",
    "plt.title('Silhouette Score vs n_clusters (KMeans)')\n",
    "\n",
    "# Label the x-axis as 'n_clusters' (number of clusters for KMeans)\n",
    "plt.xlabel('n_clusters')\n",
    "\n",
    "# Label the y-axis as 'Silhouette Score'\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "# Enable grid lines for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the legend to indicate the line represents KMeans\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-Step: Stratified Sampling by Geohash (Before vs After) \n",
    "# We'll assume you want to compare the full dataset vs a stratified sampled version — e.g., 30% of each geohash group.\n",
    "\n",
    "# 1. Plot Full Dataset (Before Sampling)\n",
    "plt.figure(figsize=(10, 6))  # Create a new figure with a defined size for better visibility.\n",
    "\n",
    "# Plot a scatter plot showing data distribution by geohash\n",
    "sns.scatterplot(\n",
    "    data=features_df,          # Use the full features dataset\n",
    "    x='longitude',             # Set x-axis to longitude\n",
    "    y='latitude',              # Set y-axis to latitude\n",
    "    hue='geohash',             # Color points by geohash to show geographic grouping\n",
    "    legend=False,              # Hide the legend for clarity if there are many geohash groups\n",
    "    s=20                       # Set marker size for better visualization\n",
    ")\n",
    "\n",
    "plt.title(\"Full Dataset: Geographical Distribution by Geohash\")  # Add plot title\n",
    "plt.xlabel(\"Longitude\")  # Label x-axis\n",
    "plt.ylabel(\"Latitude\")   # Label y-axis\n",
    "plt.show()               # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPuDXW457jbr77ZJzGjuri",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
