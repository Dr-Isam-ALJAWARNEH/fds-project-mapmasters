# Step 1: Mount Drive & Load AQ and Taxi Data
# Mount Google Drive to access files stored in the Drive
drive.mount('/content/drive')
# Set the path to the directory where the datasets are stored
base_path = '/content/drive/MyDrive/Datasets/'


# Combine all individual AQ DataFrames into a single DataFrame
aq_df = pd.concat(aq_dfs, ignore_index=True)

# Display a preview of the first few records of the combined AQ data
print(" Preview of Combined AQ Data:")
print(aq_df.head())

# Print the total number of AQ records loaded
print(f" Total AQ Records Loaded: {len(aq_df)}")

# Load All Taxi Trips Data
# Construct the full file path to the Taxi_Trips dataset
taxi_file_path = os.path.join(base_path, 'Taxi_Trips.csv')

# Load selected columns from the taxi dataset for memory efficiency
taxi_df = pd.read_csv(taxi_file_path, usecols=[
    'Trip Start Timestamp',
    'Trip Seconds',
    'Trip Miles',
    'Pickup Centroid Latitude',
    'Pickup Centroid Longitude'
])

# Rename the columns for consistency with other datasets
taxi_df = taxi_df.rename(columns={
    'Trip Start Timestamp': 'trip_start_timestamp',
    'Pickup Centroid Latitude': 'pickup_latitude',
    'Pickup Centroid Longitude': 'pickup_longitude',
    'Trip Seconds': 'trip_seconds',
    'Trip Miles': 'trip_miles'
})

# Display a preview of the taxi data
print("\n Preview of Taxi Trips Data:")
print(taxi_df.head())

# Print the total number of taxi trip records loaded
print(f" Total Taxi Records Loaded: {len(taxi_df)}")

# Step 4: Compute Weighted Feature for Clustering

# Define the weight (lambda) that determines the importance of air quality in the combined feature
lambda_ = 0.6  # Weight for air quality influence (60% AQ, 40% taxi density)

# Create a new weighted feature that blends PM2.5 air quality and taxi density
# This will be used as a combined metric for clustering
features_df['weighted_feature'] = lambda_ * features_df['pm25'] + (1 - lambda_) * features_df['taxi_density']

# Decode the geohash back into latitude and longitude coordinates
# This step is useful for visualizing or mapping the clusters later
features_df[['latitude', 'longitude']] = features_df['geohash'].apply(lambda g: pd.Series(pgh.decode(g)))