{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the pygeohash library, which allows encoding and decoding of geohashes (used for spatial indexing and grouping)\n",
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5ahOQs7Q-KC"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "# os is used for handling file paths and directory operations\n",
    "import os\n",
    "# pandas is used for data manipulation and analysis using DataFrames\n",
    "import pandas as pd\n",
    "# numpy provides support for numerical operations and array handling\n",
    "import numpy as np\n",
    "# geopandas extends pandas to handle geospatial data using shapefiles, geometry columns, etc.\n",
    "import geopandas as gpd\n",
    "# DBSCAN is a clustering algorithm from scikit-learn used for density-based clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "# MinMaxScaler is used to scale features to a specified range, typically [0, 1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# mean_absolute_error is used to evaluate prediction performance by calculating average absolute errors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# great_circle is used to calculate the shortest distance between two points on the Earth's surface\n",
    "from geopy.distance import great_circle\n",
    "# pygeohash is used for encoding latitude and longitude into geohash strings for spatial grouping\n",
    "import pygeohash as pgh\n",
    "# folium is used for interactive map visualization in Python\n",
    "import folium\n",
    "# MarkerCluster allows grouping markers in folium maps for better readability\n",
    "from folium.plugins import MarkerCluster\n",
    "# matplotlib is a standard plotting library for creating static, animated, and interactive visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# seaborn is a statistical data visualization library based on matplotlib, providing enhanced plots\n",
    "import seaborn as sns\n",
    "# shapely.geometry.Point is used to create point objects for spatial operations\n",
    "from shapely.geometry import Point\n",
    "# nearest_points is used to find the nearest point between two geometries\n",
    "from shapely.ops import nearest_points\n",
    "# google.colab.drive is used to mount Google Drive in Colab to access files stored there\n",
    "from google.colab import drive\n",
    "# Calculates the Silhouette Score, which is used to evaluate the quality of clustering results. A higher score indicates better-defined clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Splits the dataset into training and testing subsets. This is typically used in machine learning to evaluate model performance by training on one set and testing on another.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import folium for interactive map visualization of spatial data\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Drive & Load AQ and Taxi Data\n",
    "# Mount Google Drive to access files stored in the Drive\n",
    "drive.mount('/content/drive')\n",
    "# Set the path to the directory where the datasets are stored\n",
    "base_path = '/content/drive/MyDrive/Datasets/'\n",
    "\n",
    "# Initialize an empty list to collect data from each AQ CSV file\n",
    "aq_dfs = []\n",
    "# Loop through AQ_data_1.csv to AQ_data_19.csv to read each file\n",
    "for i in range(1, 20):\n",
    "    # Construct the file path for each AQ dataset\n",
    "    file_path = os.path.join(base_path, f\"AQ_data_{i}.csv\")\n",
    "\n",
    "    # Check if the file exists at the specified path\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename the necessary columns for standardization\n",
    "        df = df.rename(columns={\n",
    "            'Latitude': 'latitude',\n",
    "            'Longitude': 'longitude',\n",
    "            'ReadingDateTimeUTC': 'timestamp',\n",
    "            'PM25': 'pm25'\n",
    "        })\n",
    "\n",
    "        # Keep only the relevant columns needed for analysis\n",
    "        df = df[['latitude', 'longitude', 'timestamp', 'pm25']]\n",
    "\n",
    "        # Print how many records were loaded from the current file\n",
    "        print(f\" Loaded AQ_data_{i}.csv with {len(df)} records\")\n",
    "\n",
    "        # Append the DataFrame to the AQ list\n",
    "        aq_dfs.append(df)\n",
    "    else:\n",
    "        # If the file doesn't exist, print a message\n",
    "        print(f\" File not found: {file_path}\")\n",
    "\n",
    "# Combine all individual AQ DataFrames into a single DataFrame\n",
    "aq_df = pd.concat(aq_dfs, ignore_index=True)\n",
    "\n",
    "# Display a preview of the first few records of the combined AQ data\n",
    "print(\" Preview of Combined AQ Data:\")\n",
    "print(aq_df.head())\n",
    "\n",
    "# Print the total number of AQ records loaded\n",
    "print(f\" Total AQ Records Loaded: {len(aq_df)}\")\n",
    "\n",
    "# Load All Taxi Trips Data\n",
    "# Construct the full file path to the Taxi_Trips dataset\n",
    "taxi_file_path = os.path.join(base_path, 'Taxi_Trips.csv')\n",
    "\n",
    "# Load selected columns from the taxi dataset for memory efficiency\n",
    "taxi_df = pd.read_csv(taxi_file_path, usecols=[\n",
    "    'Trip Start Timestamp',\n",
    "    'Trip Seconds',\n",
    "    'Trip Miles',\n",
    "    'Pickup Centroid Latitude',\n",
    "    'Pickup Centroid Longitude'\n",
    "])\n",
    "\n",
    "# Rename the columns for consistency with other datasets\n",
    "taxi_df = taxi_df.rename(columns={\n",
    "    'Trip Start Timestamp': 'trip_start_timestamp',\n",
    "    'Pickup Centroid Latitude': 'pickup_latitude',\n",
    "    'Pickup Centroid Longitude': 'pickup_longitude',\n",
    "    'Trip Seconds': 'trip_seconds',\n",
    "    'Trip Miles': 'trip_miles'\n",
    "})\n",
    "\n",
    "# Display a preview of the taxi data\n",
    "print(\"\\n Preview of Taxi Trips Data:\")\n",
    "print(taxi_df.head())\n",
    "\n",
    "# Print the total number of taxi trip records loaded\n",
    "print(f\" Total Taxi Records Loaded: {len(taxi_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPET_6P_R1OR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert Timestamps & Resample by 5-Hour Windows\n",
    "# Convert AQ 'timestamp' column from string format to pandas datetime format\n",
    "aq_df['timestamp'] = pd.to_datetime(aq_df['timestamp'])\n",
    "\n",
    "# Print a few converted AQ timestamps to verify the format\n",
    "print(\"AQ timestamps converted:\")\n",
    "print(aq_df[['timestamp']].head())\n",
    "\n",
    "# Convert Taxi 'trip_start_timestamp' column from string to datetime format\n",
    "taxi_df['trip_start_timestamp'] = pd.to_datetime(taxi_df['trip_start_timestamp'])\n",
    "\n",
    "# Print a few converted Taxi timestamps to verify the format\n",
    "print(\"\\nTaxi trip timestamps converted:\")\n",
    "print(taxi_df[['trip_start_timestamp']].head())\n",
    "\n",
    "# Round each AQ timestamp down to the nearest 5-hour interval (e.g., 00:00, 05:00, 10:00...)\n",
    "aq_df['time_block'] = aq_df['timestamp'].dt.floor('5H')\n",
    "\n",
    "# Show the original and new 5-hour rounded timestamps for AQ data\n",
    "print(\"\\nAQ data after assigning to 5-hour time blocks:\")\n",
    "print(aq_df[['timestamp', 'time_block']].head())\n",
    "\n",
    "# Round each Taxi trip timestamp to the nearest 5-hour block\n",
    "taxi_df['time_block'] = taxi_df['trip_start_timestamp'].dt.floor('5H')\n",
    "\n",
    "# Show original and new 5-hour rounded timestamps for Taxi data\n",
    "print(\"\\nTaxi data after assigning to 5-hour time blocks:\")\n",
    "print(taxi_df[['trip_start_timestamp', 'time_block']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Geohash Grouping and Feature Aggregation\n",
    "# Define a helper function to compute geohashes based on latitude and longitude\n",
    "def geohash_group(df, lat_col, lon_col, precision=5):\n",
    "    \"\"\"\n",
    "    Adds a 'geohash' column to the DataFrame based on latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        lat_col (str): Name of the latitude column\n",
    "        lon_col (str): Name of the longitude column\n",
    "        precision (int): Geohash precision level (higher = smaller area)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'geohash' column added\n",
    "    \"\"\"\n",
    "    # Apply geohash encoding to each row using specified lat/lon columns\n",
    "    df['geohash'] = df.apply(lambda row: pgh.encode(row[lat_col], row[lon_col], precision=precision), axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply geohash encoding to AQ data\n",
    "aq_df = geohash_group(aq_df, 'latitude', 'longitude')\n",
    "\n",
    "# Apply geohash encoding to Taxi data\n",
    "taxi_df = geohash_group(taxi_df, 'pickup_latitude', 'pickup_longitude')\n",
    "\n",
    "# Print a sample of AQ geohash assignments\n",
    "print(\" Sample AQ geohashes:\")\n",
    "print(aq_df[['latitude', 'longitude', 'geohash']].head())\n",
    "\n",
    "# Print a sample of Taxi geohash assignments\n",
    "print(\"\\n Sample Taxi geohashes:\")\n",
    "print(taxi_df[['pickup_latitude', 'pickup_longitude', 'geohash']].head())\n",
    "\n",
    "# Aggregate taxi data: count number of trips per geohash and time block (trip density)\n",
    "taxi_density = taxi_df.groupby(['geohash', 'time_block']).size().reset_index(name='taxi_density')\n",
    "print(\"\\n Taxi density (trips per geohash and time block):\")\n",
    "print(taxi_density.head())\n",
    "\n",
    "# Aggregate AQ data: calculate average PM2.5 per geohash and time block\n",
    "aq_avg = aq_df.groupby(['geohash', 'time_block'])['pm25'].mean().reset_index(name='pm25')\n",
    "print(\"\\n Average PM2.5 per geohash and time block:\")\n",
    "print(aq_avg.head())\n",
    "\n",
    "# Merge the two datasets on common geohash and time block to align spatial-temporal features\n",
    "features_df = pd.merge(taxi_density, aq_avg, on=['geohash', 'time_block'])\n",
    "\n",
    "# Print the merged features combining taxi activity and air quality\n",
    "print(\"\\n Merged Features DataFrame (Taxi + AQ):\")\n",
    "print(features_df.head())\n",
    "\n",
    "# Show total number of records in the final dataset\n",
    "print(f\" Total Records in Feature Set: {len(features_df)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPuDXW457jbr77ZJzGjuri",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
