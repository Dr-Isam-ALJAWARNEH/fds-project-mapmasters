{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the pygeohash library, which allows encoding and decoding of geohashes (used for spatial indexing and grouping)\n",
    "!pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5ahOQs7Q-KC"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "# os is used for handling file paths and directory operations\n",
    "import os\n",
    "# pandas is used for data manipulation and analysis using DataFrames\n",
    "import pandas as pd\n",
    "# numpy provides support for numerical operations and array handling\n",
    "import numpy as np\n",
    "# geopandas extends pandas to handle geospatial data using shapefiles, geometry columns, etc.\n",
    "import geopandas as gpd\n",
    "# DBSCAN is a clustering algorithm from scikit-learn used for density-based clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "# MinMaxScaler is used to scale features to a specified range, typically [0, 1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# mean_absolute_error is used to evaluate prediction performance by calculating average absolute errors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# great_circle is used to calculate the shortest distance between two points on the Earth's surface\n",
    "from geopy.distance import great_circle\n",
    "# pygeohash is used for encoding latitude and longitude into geohash strings for spatial grouping\n",
    "import pygeohash as pgh\n",
    "# folium is used for interactive map visualization in Python\n",
    "import folium\n",
    "# MarkerCluster allows grouping markers in folium maps for better readability\n",
    "from folium.plugins import MarkerCluster\n",
    "# matplotlib is a standard plotting library for creating static, animated, and interactive visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# seaborn is a statistical data visualization library based on matplotlib, providing enhanced plots\n",
    "import seaborn as sns\n",
    "# shapely.geometry.Point is used to create point objects for spatial operations\n",
    "from shapely.geometry import Point\n",
    "# nearest_points is used to find the nearest point between two geometries\n",
    "from shapely.ops import nearest_points\n",
    "# google.colab.drive is used to mount Google Drive in Colab to access files stored there\n",
    "from google.colab import drive\n",
    "# Calculates the Silhouette Score, which is used to evaluate the quality of clustering results. A higher score indicates better-defined clusters.\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Splits the dataset into training and testing subsets. This is typically used in machine learning to evaluate model performance by training on one set and testing on another.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import folium for interactive map visualization of spatial data\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Drive & Load AQ and Taxi Data\n",
    "# Mount Google Drive to access files stored in the Drive\n",
    "drive.mount('/content/drive')\n",
    "# Set the path to the directory where the datasets are stored\n",
    "base_path = '/content/drive/MyDrive/Datasets/'\n",
    "\n",
    "# Initialize an empty list to collect data from each AQ CSV file\n",
    "aq_dfs = []\n",
    "# Loop through AQ_data_1.csv to AQ_data_19.csv to read each file\n",
    "for i in range(1, 20):\n",
    "    # Construct the file path for each AQ dataset\n",
    "    file_path = os.path.join(base_path, f\"AQ_data_{i}.csv\")\n",
    "\n",
    "    # Check if the file exists at the specified path\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename the necessary columns for standardization\n",
    "        df = df.rename(columns={\n",
    "            'Latitude': 'latitude',\n",
    "            'Longitude': 'longitude',\n",
    "            'ReadingDateTimeUTC': 'timestamp',\n",
    "            'PM25': 'pm25'\n",
    "        })\n",
    "\n",
    "        # Keep only the relevant columns needed for analysis\n",
    "        df = df[['latitude', 'longitude', 'timestamp', 'pm25']]\n",
    "\n",
    "        # Print how many records were loaded from the current file\n",
    "        print(f\" Loaded AQ_data_{i}.csv with {len(df)} records\")\n",
    "\n",
    "        # Append the DataFrame to the AQ list\n",
    "        aq_dfs.append(df)\n",
    "    else:\n",
    "        # If the file doesn't exist, print a message\n",
    "        print(f\" File not found: {file_path}\")\n",
    "\n",
    "# Combine all individual AQ DataFrames into a single DataFrame\n",
    "aq_df = pd.concat(aq_dfs, ignore_index=True)\n",
    "\n",
    "# Display a preview of the first few records of the combined AQ data\n",
    "print(\" Preview of Combined AQ Data:\")\n",
    "print(aq_df.head())\n",
    "\n",
    "# Print the total number of AQ records loaded\n",
    "print(f\" Total AQ Records Loaded: {len(aq_df)}\")\n",
    "\n",
    "# Load All Taxi Trips Data\n",
    "# Construct the full file path to the Taxi_Trips dataset\n",
    "taxi_file_path = os.path.join(base_path, 'Taxi_Trips.csv')\n",
    "\n",
    "# Load selected columns from the taxi dataset for memory efficiency\n",
    "taxi_df = pd.read_csv(taxi_file_path, usecols=[\n",
    "    'Trip Start Timestamp',\n",
    "    'Trip Seconds',\n",
    "    'Trip Miles',\n",
    "    'Pickup Centroid Latitude',\n",
    "    'Pickup Centroid Longitude'\n",
    "])\n",
    "\n",
    "# Rename the columns for consistency with other datasets\n",
    "taxi_df = taxi_df.rename(columns={\n",
    "    'Trip Start Timestamp': 'trip_start_timestamp',\n",
    "    'Pickup Centroid Latitude': 'pickup_latitude',\n",
    "    'Pickup Centroid Longitude': 'pickup_longitude',\n",
    "    'Trip Seconds': 'trip_seconds',\n",
    "    'Trip Miles': 'trip_miles'\n",
    "})\n",
    "\n",
    "# Display a preview of the taxi data\n",
    "print(\"\\n Preview of Taxi Trips Data:\")\n",
    "print(taxi_df.head())\n",
    "\n",
    "# Print the total number of taxi trip records loaded\n",
    "print(f\" Total Taxi Records Loaded: {len(taxi_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPET_6P_R1OR"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPuDXW457jbr77ZJzGjuri",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
