# Step 3: Geohash Grouping and Feature Aggregation
def geohash_group(df, lat_col, lon_col, precision=5):
    """
    Adds a 'geohash' column to the DataFrame based on latitude and longitude.

    Args:
        df (pd.DataFrame): Input DataFrame
        lat_col (str): Name of the latitude column
        lon_col (str): Name of the longitude column
        precision (int): Geohash precision level (higher = smaller area)

    Returns:
        pd.DataFrame: DataFrame with 'geohash' column added
    """
    # Apply geohash encoding to each row using specified lat/lon columns
    df['geohash'] = df.apply(lambda row: pgh.encode(row[lat_col], row[lon_col], precision=precision), axis=1)
    return df

# Apply geohash encoding to AQ data

# Apply geohash encoding to Taxi data
taxi_df = geohash_group(taxi_df, 'pickup_latitude', 'pickup_longitude')

# Print a sample of AQ geohash assignments
print(" Sample AQ geohashes:")
print(aq_df[['latitude', 'longitude', 'geohash']].head())

# Print a sample of Taxi geohash assignments
print("\n Sample Taxi geohashes:")
print(taxi_df[['pickup_latitude', 'pickup_longitude', 'geohash']].head())

# Aggregate taxi data: count number of trips per geohash and time block (trip density)
taxi_density = taxi_df.groupby(['geohash', 'time_block']).size().reset_index(name='taxi_density')
print("\n Taxi density (trips per geohash and time block):")
print(taxi_density.head())

# Aggregate AQ data: calculate average PM2.5 per geohash and time block
aq_avg = aq_df.groupby(['geohash', 'time_block'])['pm25'].mean().reset_index(name='pm25')
print("\n Average PM2.5 per geohash and time block:")
print(aq_avg.head())

# Merge the two datasets on common geohash and time block to align spatial-temporal features
features_df = pd.merge(taxi_density, aq_avg, on=['geohash', 'time_block'])

# Print the merged features combining taxi activity and air quality
print("\n Merged Features DataFrame (Taxi + AQ):")
print(features_df.head())

# Show total number of records in the final dataset
print(f" Total Records in Feature Set: {len(features_df)}")

# Step 5: Normalize and Cluster Using DBSCAN and KMeans + Evaluate
# Step 5.1: Select features for clustering
# We're clustering based on longitude, latitude, and a weighted feature (e.g., combining taxi density + PM2.5)
X = features_df[['longitude', 'latitude', 'weighted_feature']].values

# Step 5.2: Normalize the features to a [0,1] range using Min-Max Scaling
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Step 5.3: Apply DBSCAN clustering
# DBSCAN is good for finding arbitrarily shaped clusters and dealing with noise
dbscan = DBSCAN(eps=0.05, min_samples=5)
features_df['dbscan_cluster'] = dbscan.fit_predict(X_scaled)

# Step 5.4: Apply KMeans clustering for comparison
# KMeans assumes spherical clusters and needs a fixed number of clusters (n_clusters)
kmeans = KMeans(n_clusters=8, random_state=42, n_init='auto')  # You can tune n_clusters
features_df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)